<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>A Brief Introduction to enpls &bull; enpls</title><!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/united/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous"><script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous"><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet"><script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">enpls</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav"><li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../articles/index.html">Articles</a>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul><ul class="nav navbar-nav navbar-right"><li>
  <a href="https://github.com/road2stat/enpls">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul></div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>A Brief Introduction to enpls</h1>
                        <h4 class="author">Nan Xiao &lt;<a href="http://nanx.me" class="uri">http://nanx.me</a>&gt;</h4>
            
            <h4 class="date">2017-01-02</h4>
          </div>

    
    
<div class="contents">
<div id="introduction" class="section level1">
<h1 class="hasAnchor"><html><body><a href="#introduction" class="anchor"> </a></body></html>Introduction</h1>
<p>The <code>enpls</code> package offers an algorithmic framework for measuring feature importance, detecting outliers, and ensemble modeling based on (sparse) partial least squares regression. The key functions included in the package are listed in the table below.</p>
<table class="table"><colgroup><col width="37%"><col width="27%"><col width="35%"></colgroup><thead><tr class="header"><th>Task</th>
<th>Partial Least Squares</th>
<th>Sparse Partial Least Squares</th>
</tr></thead><tbody><tr class="odd"><td><p>Model fitting</p></td>
<td><p><code><a href="../reference/enpls.fit.html">enpls.fit()</a></code></p></td>
<td><p><code><a href="../reference/enspls.fit.html">enspls.fit()</a></code></p></td>
</tr><tr class="even"><td><p>Cross validation</p></td>
<td><p><code><a href="../reference/cv.enpls.html">cv.enpls()</a></code></p></td>
<td><p><code><a href="../reference/cv.enspls.html">cv.enspls()</a></code></p></td>
</tr><tr class="odd"><td><p>Detect outliers</p></td>
<td><p><code><a href="../reference/enpls.od.html">enpls.od()</a></code></p></td>
<td><p><code><a href="../reference/enspls.od.html">enspls.od()</a></code></p></td>
</tr><tr class="even"><td><p>Measure feature importance</p></td>
<td><p><code><a href="../reference/enpls.fs.html">enpls.fs()</a></code></p></td>
<td><p><code><a href="../reference/enspls.fs.html">enspls.fs()</a></code></p></td>
</tr><tr class="odd"><td><p>Evaluate applicability domain</p></td>
<td><p><code><a href="../reference/enpls.ad.html">enpls.ad()</a></code></p></td>
<td><p><code><a href="../reference/enspls.ad.html">enspls.ad()</a></code></p></td>
</tr></tbody></table><p>Please refer to our paper <span class="citation">(Xiao et al. 2016)</span> for details on the intuition and mathematical details of the ensemble learning approaches used here.</p>
<p>In the next, we will use the data from <span class="citation">(Wang et al. 2015)</span> to demonstrate the general workflow of <code>enpls</code>. The dataset contains 1,000 compounds, each characterized by 80 molecular descriptors. The response is the octanol/water partition coefficient at pH 7.4 (logD7.4).</p>
<p>Let&rsquo;s load the data and take a look at it:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">"enpls"</span>)
<span class="kw">library</span>(<span class="st">"ggplot2"</span>)

<span class="kw">data</span>(<span class="st">"logd1k"</span>)
x =<span class="st"> </span>logd1k$x
y =<span class="st"> </span>logd1k$y
<span class="kw">head</span>(x)[, <span class="dv">1</span>:<span class="dv">5</span>]</code></pre></div>
<pre><code>##   BalabanJ  BertzCT   Chi0  Chi0n  Chi0v
## 1    1.949  882.760 16.845 13.088 13.088
## 2    1.970  781.936 15.905 13.204 14.021
## 3    2.968  343.203  9.845  7.526  7.526
## 4    2.050 1133.679 19.836 15.406 15.406
## 5    2.719  437.346 12.129  9.487  9.487
## 6    2.031  983.304 19.292 15.289 15.289</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(y)</code></pre></div>
<pre><code>## [1] -0.96 -0.92 -0.90 -0.83 -0.82 -0.79</code></pre>
</div>
<div id="model-fitting" class="section level1">
<h1 class="hasAnchor"><html><body><a href="#model-fitting" class="anchor"> </a></body></html>Model Fitting</h1>
<p>Here we fit the ensemble sparse partial least squares to the data, so that the model complexity could usually be further reduced than vanilla partial least squares when we build each model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">42</span>)
fit =<span class="st"> </span><span class="kw"><a href="../reference/enspls.fit.html">enspls.fit</a></span>(x, y, <span class="dt">ratio =</span> <span class="fl">0.7</span>, <span class="dt">reptimes =</span> <span class="dv">20</span>, <span class="dt">maxcomp =</span> <span class="dv">3</span>)
y.pred =<span class="st"> </span><span class="kw">predict</span>(fit, <span class="dt">newx =</span> x)

df =<span class="st"> </span><span class="kw">data.frame</span>(y, y.pred)
<span class="kw">ggplot</span>(df, <span class="kw">aes_string</span>(<span class="dt">x =</span> <span class="st">"y"</span>, <span class="dt">y =</span> <span class="st">"y.pred"</span>)) +
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">colour =</span> <span class="st">"darkgrey"</span>) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">3</span>, <span class="dt">shape =</span> <span class="dv">1</span>, <span class="dt">alpha =</span> <span class="fl">0.8</span>) +
<span class="st">  </span><span class="kw">coord_fixed</span>(<span class="dt">ratio =</span> <span class="dv">1</span>) +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">"Observed Response"</span>) +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">"Predicted Response"</span>)</code></pre></div>
<p><img src="enpls_files/figure-html/unnamed-chunk-2-1.png" width="600" height="600"></p>
<p>We used the fitted model to predict on the training data and plotted the predicted values against the true values.</p>
<p>The parameter <code>ratio</code> decides the sampling ratio for each Monte-Carlo run; <code>maxcomp</code> controls the maximum number of components included within each model; <code>reptimes</code> sets the times of Monte-Carlo resampling, we recommend setting it to a large number (500 by default).</p>
<p>One common parameter for all functions in <code>enpls</code> is <code>parallel</code>, it controls the number of CPU cores to use if you want to train the models in parallel.</p>
</div>
<div id="cross-validation" class="section level1">
<h1 class="hasAnchor"><html><body><a href="#cross-validation" class="anchor"> </a></body></html>Cross Validation</h1>
<p>K-fold cross validation is a traditional way to measure the empirical predictive performance of the model. We can use function <code><a href="../reference/cv.enpls.html">cv.enpls()</a></code> or <code><a href="../reference/cv.enspls.html">cv.enspls()</a></code> to perform <span class="math inline">\(k\)</span>-fold cross validation for the ensemble (sparse) partial least squares model.</p>
<p>Since the parameters (number of components and level of sparsity) are automatically tuned for each model in <code>enpls</code>, the cross validation here is used to see if certain combinations of parameters (specified by <code>ratio</code>, <code>maxcomp</code>, <code>alpha</code>, etc.) can produce ensemble models with better performance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv.fit =<span class="st"> </span><span class="kw"><a href="../reference/cv.enspls.html">cv.enspls</a></span>(x, y, <span class="dt">nfolds =</span> <span class="dv">5</span>, <span class="dt">ratio =</span> <span class="fl">0.7</span>,
                   <span class="dt">reptimes =</span> <span class="dv">10</span>, <span class="dt">maxcomp =</span> <span class="dv">3</span>, <span class="dt">verbose =</span> <span class="ot">FALSE</span>)
<span class="kw">print</span>(cv.fit)</code></pre></div>
<pre><code>## Cross Validation Result for Ensemble Sparse Partial Least Squares
## ---
## RMSE = 1.1425
## MAE = 0.914457
## Rsquare = 0.586869</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(cv.fit)</code></pre></div>
<p><img src="enpls_files/figure-html/unnamed-chunk-3-1.png" width="600" height="600"></p>
<p>The returned object gives three model performance evaluation metrics for the ensemble model: RMSE, MAE, and <span class="math inline">\(R^2\)</span>. Here we also plotted the predicted values for each test fold against the true response.</p>
</div>
<div id="feature-importance" class="section level1">
<h1 class="hasAnchor"><html><body><a href="#feature-importance" class="anchor"> </a></body></html>Feature Importance</h1>
<p>To measure feature importance, simply use <code><a href="../reference/enpls.fs.html">enpls.fs()</a></code> or <code><a href="../reference/enspls.fs.html">enspls.fs()</a></code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fs =<span class="st"> </span><span class="kw"><a href="../reference/enspls.fs.html">enspls.fs</a></span>(x, y, <span class="dt">ratio =</span> <span class="fl">0.7</span>, <span class="dt">reptimes =</span> <span class="dv">20</span>, <span class="dt">maxcomp =</span> <span class="dv">3</span>)
<span class="kw">print</span>(fs, <span class="dt">nvar =</span> <span class="dv">10</span>)</code></pre></div>
<pre><code>## Variable Importance by Ensemble Sparse Partial Least Squares
## ---
##                     Importance
## Chi1n                 30.24386
## NumValenceElectrons   28.36228
## LabuteASA             26.43732
## MolMR                 23.53262
## Chi1                  22.10168
## HeavyAtomCount        21.69942
## Chi0n                 19.77486
## TPSA                  18.56782
## Chi0v                 18.05993
## Chi0                  17.40688</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fs, <span class="dt">nvar =</span> <span class="dv">10</span>)</code></pre></div>
<p><img src="enpls_files/figure-html/unnamed-chunk-4-1.png" width="600" height="600"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fs, <span class="dt">type =</span> <span class="st">"boxplot"</span>, <span class="dt">nvar =</span> <span class="dv">10</span>)</code></pre></div>
<p><img src="enpls_files/figure-html/unnamed-chunk-4-2.png" width="600" height="600"></p>
<p>The top 10 most important features are ranked as above. The boxplot gives additional information about the coefficient stability of each feature. We can see the feature TPSA (Topological Polar Surface Area) has different pattern compared to others: it has large effect size, but the effect sizes also have a large variance. This indicates that TPSA is important for predicting logD7.4. However, such importance may vary on different subsets of the samples.</p>
</div>
<div id="outlier-detection" class="section level1">
<h1 class="hasAnchor"><html><body><a href="#outlier-detection" class="anchor"> </a></body></html>Outlier Detection</h1>
<p>By using information from the prediction error distribution for each sample produced by many models, we can measure if the responses of particular samples are harder to predict than the others. Such measurements can help on identifying outliers in the dataset. Thus, they can be removed to get us a &ldquo;clean&rdquo; dataset before the actual modeling.</p>
<p>This could be done with <code><a href="../reference/enpls.od.html">enpls.od()</a></code>, <code><a href="../reference/enspls.od.html">enspls.od()</a></code> easily:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">od =<span class="st"> </span><span class="kw"><a href="../reference/enspls.od.html">enspls.od</a></span>(x, y, <span class="dt">ratio =</span> <span class="fl">0.8</span>, <span class="dt">reptimes =</span> <span class="dv">20</span>, <span class="dt">maxcomp =</span> <span class="dv">3</span>)
<span class="kw">plot</span>(od, <span class="dt">prob =</span> <span class="fl">0.05</span>)</code></pre></div>
<p><img src="enpls_files/figure-html/unnamed-chunk-5-1.png" width="600" height="600"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(od, <span class="dt">criterion =</span> <span class="st">"sd"</span>, <span class="dt">sdtimes =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="enpls_files/figure-html/unnamed-chunk-5-2.png" width="600" height="600"></p>
<p>The two plots showed that several samples in our dataset might be outlier candidates, based on two different criterions. The samples in each area of the plots represent different types of outliers, as defined in <span class="citation">(Xiao et al. 2016)</span>.</p>
</div>
<div id="applicability-domain-evaluation" class="section level1">
<h1 class="hasAnchor"><html><body><a href="#applicability-domain-evaluation" class="anchor"> </a></body></html>Applicability Domain Evaluation</h1>
<p>Model applicability domain measures how well the predictive model (PLS/Sparse PLS model) we built on the training set performs on external test sets. A certain type of perturbation (such as bootstrapping, jackknifing) is applied to the samples or the variables of the training set, and we could get many different predictions for all samples in all test sets (and of course, including the training set) using each sub-model built with each perturbated training set. The general evaluation strategy design and comparisons were analyzed in <span class="citation">Kaneko and Funatsu (2014)</span>.</p>
<p>The functions <code><a href="../reference/enpls.ad.html">enpls.ad()</a></code> and <code><a href="../reference/enspls.ad.html">enspls.ad()</a></code> could help us evaluate the model applicability domain. Here we constructed two &ldquo;pseudo&rdquo; test sets from the original <code>logd1k</code> dataset for demonstration:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># remove low variance variables</span>
x =<span class="st"> </span>x[, -<span class="kw">c</span>(<span class="dv">17</span>, <span class="dv">52</span>, <span class="dv">59</span>)]

<span class="co"># make training set</span>
x.tr =<span class="st"> </span>x[<span class="dv">1</span>:<span class="dv">500</span>, ]
y.tr =<span class="st"> </span>y[<span class="dv">1</span>:<span class="dv">500</span>]

<span class="co"># make two test sets</span>
x.te =<span class="st"> </span><span class="kw">list</span>(<span class="st">"test.1"</span> =<span class="st"> </span>x[<span class="dv">501</span>:<span class="dv">700</span>, ],
            <span class="st">"test.2"</span> =<span class="st"> </span>x[<span class="dv">701</span>:<span class="dv">800</span>, ])
y.te =<span class="st"> </span><span class="kw">list</span>(<span class="st">"test.1"</span> =<span class="st"> </span>y[<span class="dv">501</span>:<span class="dv">700</span>],
            <span class="st">"test.2"</span> =<span class="st"> </span>y[<span class="dv">701</span>:<span class="dv">800</span>])

ad =<span class="st"> </span><span class="kw"><a href="../reference/enspls.ad.html">enspls.ad</a></span>(x.tr, y.tr, x.te, y.te,
               <span class="dt">maxcomp =</span> <span class="dv">3</span>, <span class="dt">space =</span> <span class="st">"variable"</span>, <span class="dt">method =</span> <span class="st">"mc"</span>,
               <span class="dt">ratio =</span> <span class="fl">0.8</span>, <span class="dt">reptimes =</span> <span class="dv">50</span>)
<span class="kw">plot</span>(ad)</code></pre></div>
<p><img src="enpls_files/figure-html/unnamed-chunk-6-1.png" width="600" height="600"></p>
<p>Additionally, by using</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(ad, <span class="dt">type =</span> <span class="st">"interactive"</span>)</code></pre></div>
<p>we will get an interactive plot, which could help us better explore the model applicability domain by supporting zooming-in / inspecting which sample each point represents interactively. The interactive plot is based on <a href="https://cran.r-project.org/package=plotly">plotly</a>, and it only requires an HTML viewer to be correctly rendered.</p>
</div>
<div id="conclusion" class="section level1">
<h1 class="hasAnchor"><html><body><a href="#conclusion" class="anchor"> </a></body></html>Conclusion</h1>
<p>Ensemble learning approaches are not only powerful for improving base learner&rsquo;s predictive performance but also capable of accomplishing model diagnostic tasks, such as measuring the importance of features. It would be interesting to see if such ideas could be applied to more relevant topics and further facilitate the predictive modeling tasks.</p>
<p>If you used <code>enpls</code> in your research, please feel free to cite our paper <span class="citation">(Xiao et al. 2016)</span> in your publications. We will be more than happy to listen to your feedbacks. The enpls project website: <a href="http://enpls.org" class="uri">http://enpls.org</a></p>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="hasAnchor"><html><body><a href="#references" class="anchor"> </a></body></html>References</h1>
<div id="refs" class="references">
<div id="ref-kaneko2014applicability">
<p>Kaneko, Hiromasa, and Kimito Funatsu. 2014. &ldquo;Applicability Domain Based on Ensemble Learning in Classification and Regression Analyses.&rdquo; <em>Journal of Chemical Information and Modeling</em> 54 (9). ACS Publications: 2469&ndash;82.</p>
</div>
<div id="ref-wang2015silico">
<p>Wang, Jian-Bing, Dong-Sheng Cao, Min-Feng Zhu, Yong-Huan Yun, Nan Xiao, and Yi-Zeng Liang. 2015. &ldquo;In Silico Evaluation of LogD7. 4 and Comparison with Other Prediction Methods.&rdquo; <em>Journal of Chemometrics</em> 29 (7). Wiley Online Library: 389&ndash;98.</p>
</div>
<div id="ref-xiao2016enpls">
<p>Xiao, Nan, Dong-Sheng Cao, Miao-Zhu Li, and Qing-Song Xu. 2016. &ldquo;enpls: an R Package for Ensemble Partial Least Squares Regression.&rdquo; <em>arXiv preprint</em>.</p>
</div>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2>Contents</h2>
      <ul class="nav nav-pills nav-stacked"><li><a href="#introduction">Introduction</a></li>
      <li><a href="#model-fitting">Model Fitting</a></li>
      <li><a href="#cross-validation">Cross Validation</a></li>
      <li><a href="#feature-importance">Feature Importance</a></li>
      <li><a href="#outlier-detection">Outlier Detection</a></li>
      <li><a href="#applicability-domain-evaluation">Applicability Domain Evaluation</a></li>
      <li><a href="#conclusion">Conclusion</a></li>
      <li><a href="#references">References</a></li>
      </ul></div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Nan Xiao, Dongsheng Cao, Miaozhu Li, Qingsong Xu.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer></div>

  </body></html>
